{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:10.509556Z",
     "start_time": "2024-04-09T10:30:03.533675Z"
    }
   },
   "id": "7393b5f251a327cd",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:10.521414Z",
     "start_time": "2024-04-09T10:30:10.511536Z"
    }
   },
   "outputs": [],
   "source": [
    "# reads in the json file, only to the max entries and returns them as json_array, if max entries is set to 0 then it reads the full thing\n",
    "def read_partial_json_file(filename, max_entries=0, encoding='utf-8'):\n",
    "    json_array = []\n",
    "    with open(filename, 'r', encoding=encoding) as file:\n",
    "        if max_entries == 0:\n",
    "            for line in file:\n",
    "                json_array.append(json.loads(line))\n",
    "        else:\n",
    "            for _ in range(max_entries):\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                json_array.append(json.loads(line))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def add_missing_keys(json_array):\n",
    "    for obj in json_array:\n",
    "        for key in ['stars', 'useful', 'funny', 'cool', 'text']:\n",
    "            if key not in obj:\n",
    "                obj[key] = 0\n",
    "                if key == 'stars':\n",
    "                    obj[key] = 3\n",
    "                print(\"Key {} not found in json\".format(key))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "# removes specified keys from json array\n",
    "def remove_keys(json_array, keys_to_remove):\n",
    "    for obj in json_array:\n",
    "        for key in keys_to_remove:\n",
    "            obj.pop(key, None)\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def ConvertJSONFileToDataFrame(filename, max_entries=1000, encoding='utf-8'):\n",
    "    #load in the json array\n",
    "    json_array = read_partial_json_file(filename, max_entries, encoding)\n",
    "    #add in the missing keys, will set to 0 for now but a heuristic for this will have to be made.\n",
    "    json_array = add_missing_keys(json_array)\n",
    "    df = pd.DataFrame(json_array)\n",
    "    ColumnsToRemove = ['business_id', 'user_id', 'date', 'review_id']\n",
    "    df = df.drop(columns=ColumnsToRemove)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ckmfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ckmfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "filename = 'yelp_academic_dataset_review.json'\n",
    "dataset = ConvertJSONFileToDataFrame(filename, 5000)\n",
    "\n",
    "# dataset.drop(dataset[(dataset.stars > 1) & (dataset.stars < 5)].index, inplace=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stem = SnowballStemmer(\"english\")\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def stemText(text):\n",
    "    return \" \".join([i for i in word_tokenize(text) if not i in stopWords])\n",
    "\n",
    "#Data preprocessing: convert text to lowercase\n",
    "X = dataset['text'].map(lambda x: stemText(x.lower()))\n",
    "#convert star count to categories starting from 0\n",
    "translation = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "labels = ['1', '2', '3', '4', '5']\n",
    "y = dataset['stars'].copy()\n",
    "y.replace(translation, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=117)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=312)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:15.069051Z",
     "start_time": "2024-04-09T10:30:10.524406Z"
    }
   },
   "id": "878e6bc58d634734",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_test_vec = vec.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:15.300139Z",
     "start_time": "2024-04-09T10:30:15.071041Z"
    }
   },
   "id": "fcf0871e119f1c19",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, inputSize, layer2size, layer3size):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, layer2size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer2size, layer3size)\n",
    "        self.fc3 = nn.Linear(layer3size, 5)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:15.309798Z",
     "start_time": "2024-04-09T10:30:15.303387Z"
    }
   },
   "id": "849b2b5faf80f5c9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_vec.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vec.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:15.753811Z",
     "start_time": "2024-04-09T10:30:15.312786Z"
    }
   },
   "id": "ba4a0312b0b5e73f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = NNClassifier(X_train_vec.shape[1], 128, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:15.799552Z",
     "start_time": "2024-04-09T10:30:15.755800Z"
    }
   },
   "id": "48e4821061ca72f4",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 94.533775806427\n",
      "Loss: 81.22799265384674\n",
      "Loss: 72.84832310676575\n",
      "Loss: 67.0091964006424\n",
      "Loss: 63.14890170097351\n",
      "Loss: 60.82495927810669\n",
      "Loss: 59.6997926235199\n",
      "Loss: 59.12156391143799\n",
      "Loss: 58.8633993268013\n",
      "Loss: 58.732957541942596\n",
      "Loss: 58.5247318148613\n",
      "Loss: 58.428956389427185\n",
      "Loss: 58.38627713918686\n",
      "Loss: 58.35429239273071\n",
      "Loss: 58.3017703294754\n",
      "Loss: 58.302753031253815\n",
      "Loss: 58.26812690496445\n",
      "Loss: 58.20780748128891\n",
      "Loss: 58.155430018901825\n",
      "Loss: 58.12505620718002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        47\n",
      "           1       0.41      0.29      0.34        41\n",
      "           2       0.36      0.34      0.35        59\n",
      "           3       0.40      0.45      0.43       119\n",
      "           4       0.73      0.72      0.72       234\n",
      "\n",
      "    accuracy                           0.58       500\n",
      "   macro avg       0.52      0.51      0.51       500\n",
      "weighted avg       0.58      0.58      0.57       500\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#choo choo\n",
    "trainDataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "trainLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainingEpochs = 20\n",
    "for i in range(trainingEpochs):\n",
    "    sumLoss = 0.0\n",
    "    for text, stars in trainLoader:\n",
    "        optimizer.zero_grad()\n",
    "        result = model(text)\n",
    "        loss = criterion(result, stars)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sumLoss += loss.item()\n",
    "    print(\"Loss: {}\".format(sumLoss))\n",
    "\n",
    "outputs = model(X_test_tensor)\n",
    "wasteTensors, predicted = torch.max(outputs, 1)\n",
    "y_test_np = y_test_tensor.numpy()\n",
    "predicted_np = predicted.numpy()\n",
    "print(classification_report(y_test_np, predicted_np))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:52.719673Z",
     "start_time": "2024-04-09T10:30:15.802583Z"
    }
   },
   "id": "d464e9e87a138e1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67        51\n",
      "           1       0.29      0.21      0.24        52\n",
      "           2       0.25      0.20      0.22        71\n",
      "           3       0.26      0.33      0.29       101\n",
      "           4       0.70      0.69      0.69       225\n",
      "\n",
      "    accuracy                           0.50       500\n",
      "   macro avg       0.43      0.42      0.42       500\n",
      "weighted avg       0.50      0.50      0.50       500\n"
     ]
    }
   ],
   "source": [
    "X_valid_vec = vec.transform(X_valid)\n",
    "X_valid_tensor = torch.tensor(X_valid_vec.toarray(), dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid.to_numpy(), dtype=torch.long)\n",
    "\n",
    "\n",
    "outputs = model(X_valid_tensor)\n",
    "wasteTensors, predicted = torch.max(outputs, 1)\n",
    "y_valid_np = y_valid_tensor.numpy()\n",
    "predicted_np = predicted.numpy()\n",
    "print(classification_report(y_valid_np, predicted_np))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:52.839050Z",
     "start_time": "2024-04-09T10:30:52.721667Z"
    }
   },
   "id": "64d8c555488e2466",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NNRegressor(nn.Module):\n",
    "    def __init__(self, inputSize, layer2size, layer3size):\n",
    "        super(NNRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputSize, layer2size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer2size, layer3size)\n",
    "        self.fc3 = nn.Linear(layer3size, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:52.848983Z",
     "start_time": "2024-04-09T10:30:52.842034Z"
    }
   },
   "id": "6a3c7798afd83e7e",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = dataset['cool']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=117)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=312)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_valid_tensor = torch.tensor(y_valid.to_numpy(), dtype=torch.float32).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:52.868026Z",
     "start_time": "2024-04-09T10:30:52.850968Z"
    }
   },
   "id": "a9e3e2d9ca3bc051",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "coolModel = NNRegressor(X_train_vec.shape[1], 128, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:52.906459Z",
     "start_time": "2024-04-09T10:30:52.870022Z"
    }
   },
   "id": "16265c7c032b5ce3",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 86.9375\n",
      "Loss: 86.59375\n",
      "Loss: 86.640625\n",
      "Loss: 88.34375\n",
      "Loss: 86.640625\n",
      "Loss: 88.5\n",
      "Loss: 86.71875\n",
      "Loss: 88.28125\n",
      "Loss: 88.90625\n",
      "Loss: 86.609375\n",
      "Loss: 86.9375\n",
      "Loss: 86.609375\n",
      "Loss: 86.59375\n",
      "Loss: 86.578125\n",
      "Loss: 86.765625\n",
      "Loss: 86.578125\n",
      "Loss: 88.203125\n",
      "Loss: 86.59375\n",
      "Loss: 86.578125\n",
      "Loss: 86.578125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "trainDataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "trainLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for i in range(trainingEpochs):\n",
    "    sumLoss = 0.0\n",
    "    for text, coolness in trainLoader:\n",
    "        optimizer.zero_grad()\n",
    "        result = coolModel(text)\n",
    "        loss = criterion(result, coolness)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sumLoss += loss.item()\n",
    "    print(\"Loss: {}\".format(sumLoss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:31:07.793231Z",
     "start_time": "2024-04-09T10:30:52.909822Z"
    }
   },
   "id": "c3cb13f4bbea2eb8",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      1.00      0.89       401\n",
      "         1.0       0.00      0.00      0.00        65\n",
      "         2.0       0.00      0.00      0.00        22\n",
      "         3.0       0.00      0.00      0.00         5\n",
      "         4.0       0.00      0.00      0.00         3\n",
      "         5.0       0.00      0.00      0.00         2\n",
      "        10.0       0.00      0.00      0.00         1\n",
      "        11.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.10      0.12      0.11       500\n",
      "weighted avg       0.64      0.80      0.71       500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Repos\\AIProject2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Repos\\AIProject2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Repos\\AIProject2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "outputs = coolModel(X_test_tensor)\n",
    "wasteTensors, predicted = torch.max(outputs, 1)\n",
    "y_test_np = y_test_tensor.numpy()\n",
    "predicted_np = predicted.numpy()\n",
    "print(classification_report(y_test_np, predicted_np))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:31:07.844334Z",
     "start_time": "2024-04-09T10:31:07.795737Z"
    }
   },
   "id": "13400cdfe68bbe66",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:31:07.850314Z",
     "start_time": "2024-04-09T10:31:07.846324Z"
    }
   },
   "id": "d2e319191004baeb",
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
