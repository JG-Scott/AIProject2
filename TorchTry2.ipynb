{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T07:24:49.876542Z",
     "start_time": "2024-04-09T07:24:49.871733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reads in the json file, only to the max entries and returns them as json_array, if max entries is set to 0 then it reads the full thing\n",
    "def read_partial_json_file(filename, max_entries=0, encoding='utf-8'):\n",
    "    json_array = []\n",
    "    with open(filename, 'r', encoding=encoding) as file:\n",
    "        if max_entries == 0:\n",
    "            for line in file:\n",
    "                json_array.append(json.loads(line))\n",
    "        else:\n",
    "            for _ in range(max_entries):\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                json_array.append(json.loads(line))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def add_missing_keys(json_array):\n",
    "    for obj in json_array:\n",
    "        for key in ['stars', 'useful', 'funny', 'cool', 'text']:\n",
    "            if key not in obj:\n",
    "                obj[key] = 0\n",
    "                if key == 'stars':\n",
    "                    obj[key] = 3\n",
    "                print(\"Key {} not found in json\".format(key))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "# removes specified keys from json array\n",
    "def remove_keys(json_array, keys_to_remove):\n",
    "    for obj in json_array:\n",
    "        for key in keys_to_remove:\n",
    "            obj.pop(key, None)\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def ConvertJSONFileToDataFrame(filename, max_entries=1000, encoding='utf-8'):\n",
    "    #load in the json array\n",
    "    json_array = read_partial_json_file(filename, max_entries, encoding)\n",
    "    #add in the missing keys, will set to 0 for now but a heuristic for this will have to be made.\n",
    "    json_array = add_missing_keys(json_array)\n",
    "    df = pd.DataFrame(json_array)\n",
    "    ColumnsToRemove = ['business_id', 'user_id', 'date', 'review_id']\n",
    "    df = df.drop(columns=ColumnsToRemove)\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T07:24:49.888002Z",
     "start_time": "2024-04-09T07:24:49.878533Z"
    }
   },
   "id": "4cb171816a6809e0",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ckmfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ckmfo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "234    best iced latte beignet tried . live music als...\n827    classic burger fries . overall pleased experie...\n565    best place go gyros ! quality food amazing . a...\n661    replaced dive watch battery less 5 minutes rea...\n781    hands favorite brewery world ! ! husband stl e...\n                             ...                        \n599    listened reviews -- kind disappointed . layout...\n337    worst service advisors ! used good kelly team ...\n80     couple friends stopped late night milkshakes f...\n112    westfall replaced roof april . could pleased e...\n494    went show seeing university city area . across...\nName: text, Length: 800, dtype: object"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'yelp_academic_dataset_review.json'\n",
    "dataset = ConvertJSONFileToDataFrame(filename, 1000)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stem = SnowballStemmer(\"english\")\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def stemText(text):\n",
    "    return \" \".join([i for i in word_tokenize(text) if not i in stopWords])\n",
    "\n",
    "#Data preprocessing: convert text to lowercase\n",
    "X = dataset['text'].map(lambda x: stemText(x.lower()))\n",
    "#convert star count to categories starting from 0\n",
    "translation = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "labels = ['1', '2', '3', '4', '5']\n",
    "y = dataset['stars'].copy()\n",
    "y.replace(translation, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=117)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=312)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T07:24:51.358222Z",
     "start_time": "2024-04-09T07:24:49.892540Z"
    }
   },
   "id": "caa2be349eb6a3af",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#I don't have a GPU on my laptop so this is untestable\n",
    "if torch.cuda.is_available():\n",
    "    torchDevice = torch.device('cuda')\n",
    "else:\n",
    "    torchDevice = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T07:24:51.367089Z",
     "start_time": "2024-04-09T07:24:51.360211Z"
    }
   },
   "id": "2a44fde38d83e649",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, vocabSize, embed_size, layer1size, layer2size, layer3size, dropout, maxWordCt):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(maxWordCt * vocabSize, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed = nn.Linear(embed_size, layer1size)\n",
    "        self.full1 = nn.Linear(layer1size, layer2size)\n",
    "        self.full2 = nn.Linear(layer2size, layer3size)\n",
    "        self.full3 = nn.Linear(layer3size, 5) #the output is just projected star count\n",
    "        \n",
    "        # #First fully connected layer\n",
    "        # self.fc1 = torch.nn.Linear(vocabSize,layer1size)\n",
    "        # #Second fully connected layer\n",
    "        # self.fc2 = torch.nn.Linear(layer1size,5)\n",
    "        # #Final output of sigmoid function      \n",
    "        # self.output = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # #return self.linear(text)\n",
    "        # fc1 = self.fc1(text)\n",
    "        # fc2 = self.fc2(fc1)\n",
    "        # output = self.output(fc2)\n",
    "        # return output[:, -1]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        x = embedded.view(embedded.shape[0], -1)\n",
    "        x = self.relu(self.feed(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.full1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.full2(x))\n",
    "        x = self.dropout(x)\n",
    "        result = self.full3(x)\n",
    "        return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T07:24:51.381159Z",
     "start_time": "2024-04-09T07:24:51.369078Z"
    }
   },
   "id": "bf11130903fc25ce",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = NeuralNet(X_train_vec.shape[1], 300, 128, 64, 32, dropout=0.5, maxWordCt=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-09T07:24:51.384730Z"
    }
   },
   "id": "e991f3bfdd3d4cd3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "48f51d85a7011dc1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
