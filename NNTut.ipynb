{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:35.341232Z",
     "start_time": "2024-04-08T18:26:35.323815Z"
    }
   },
   "id": "5ca8fab994219938",
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reads in the json file, only to the max entries and returns them as json_array, if max entries is set to 0 then it reads the full thing\n",
    "def read_partial_json_file(filename, max_entries=0, encoding='utf-8'):\n",
    "    json_array = []\n",
    "    with open(filename, 'r', encoding=encoding) as file:\n",
    "        if max_entries == 0:\n",
    "            for line in file:\n",
    "                json_array.append(json.loads(line))\n",
    "        else:\n",
    "            for _ in range(max_entries):\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                json_array.append(json.loads(line))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def add_missing_keys(json_array):\n",
    "    for obj in json_array:\n",
    "        for key in ['stars', 'useful', 'funny', 'cool', 'text']:\n",
    "            if key in obj:\n",
    "                if key == 'stars':\n",
    "                    obj[key] = obj[key]/5\n",
    "            if key not in obj:\n",
    "                obj[key] = 0\n",
    "                if key == 'stars':\n",
    "                    obj[key] = 3/5\n",
    "                print(\"Key {} not found in json\".format(key))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "# removes specified keys from json array\n",
    "def remove_keys(json_array, keys_to_remove):\n",
    "    for obj in json_array:\n",
    "        for key in keys_to_remove:\n",
    "            obj.pop(key, None)\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def ConvertJSONFileToDataFrame(filename, max_entries=1000, encoding='utf-8'):\n",
    "    #load in the json array\n",
    "    json_array = read_partial_json_file(filename, max_entries, encoding)\n",
    "    #add in the missing keys, will set to 0 for now but a heuristic for this will have to be made.\n",
    "    json_array = add_missing_keys(json_array)\n",
    "    df = pd.DataFrame(json_array)\n",
    "    ColumnsToRemove = ['business_id', 'user_id', 'date', 'review_id']\n",
    "    df = df.drop(columns=ColumnsToRemove)\n",
    "    return df\n",
    "\n",
    "\n",
    "filename = 'yelp_academic_dataset_review.json'\n",
    "df = ConvertJSONFileToDataFrame(filename)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:35.414867Z",
     "start_time": "2024-04-08T18:26:35.346220Z"
    }
   },
   "id": "c547a16d7546548e",
   "execution_count": 150
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 24532)\n",
      "(2000, 24532)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the data and then split it, keeping 20% aside for testing\n",
    "filename = 'yelp_academic_dataset_review.json'\n",
    "dataset = ConvertJSONFileToDataFrame(filename, 10000)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['stars'], test_size=0.2)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "vectorizer.fit(dataset['text'])\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "print(X_train_vec.shape)\n",
    "print(X_test_vec.shape)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:37.552753Z",
     "start_time": "2024-04-08T18:26:35.441789Z"
    }
   },
   "id": "initial_id",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_units, num_classes):\n",
    "        super().__init__()\n",
    "        #First fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(vocab_size, hidden_units)\n",
    "        #Second fully connected layer\n",
    "        self.fc2 = torch.nn.Linear(hidden_units, num_classes)\n",
    "        #Final output of sigmoid function      \n",
    "        self.output = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.fc1(x)\n",
    "        fc1 = self.dropout(fc1)\n",
    "        fc2 = self.fc2(fc1)\n",
    "        fc2 = self.dropout(fc2)\n",
    "        output = self.output(fc2)\n",
    "        return output[:, -1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:37.569322Z",
     "start_time": "2024-04-08T18:26:37.555748Z"
    }
   },
   "id": "781e837a79ae5d80",
   "execution_count": 152
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(0.8000, dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_vec.todense()).float()\n",
    "X_test_tensor = torch.from_numpy(X_test_vec.todense()).float()\n",
    "Y_train_tensor = torch.from_numpy(np.array(y_train))\n",
    "Y_test_tensor = torch.from_numpy(np.array(y_test))\n",
    "\n",
    "trainData = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "print(trainData[0])\n",
    "trainLoader = DataLoader(trainData, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:26:39.456470Z",
     "start_time": "2024-04-08T18:26:37.572317Z"
    }
   },
   "id": "1c55833bbf35db2",
   "execution_count": 153
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 4 epoch training loss is 0.5483723878860474\n",
      "6539    THE GOOD: great food for a decent price.  We g...\n",
      "8069    The tea is not that good...but the cheese crea...\n",
      "747     Friday, July 22 we decided to give this eatery...\n",
      "2430    If this spot plans to make it they need to hav...\n",
      "873     Had not been for years.\\n\\nWe went for lunch. ...\n",
      "                              ...                        \n",
      "5887    We sat on the patio on one of the most beautif...\n",
      "8119    The man who worked on me did a wonderful job. ...\n",
      "5562    I come here all the time never had a problem. ...\n",
      "7681    Excellent little old school bakery. Maybe the ...\n",
      "5864    I took my kiddos to incredible pizza today.  W...\n",
      "Name: text, Length: 2000, dtype: object\n",
      "6539    0.4\n",
      "8069    0.6\n",
      "747     0.6\n",
      "2430    0.4\n",
      "873     0.4\n",
      "       ... \n",
      "5887    1.0\n",
      "8119    1.0\n",
      "5562    0.8\n",
      "7681    1.0\n",
      "5864    0.4\n",
      "Name: stars, Length: 2000, dtype: float64\n",
      "tensor([0.7183, 0.7450, 0.7901,  ..., 0.7087, 0.5000, 0.6308],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m2 = Network(X_train_vec.shape[1], 128, 5)\n",
    "optimizer = torch.optim.SGD(m2.parameters(), lr=0.001)  #Initialize loss function\n",
    "loss_fun = nn.BCELoss(reduction='mean')\n",
    "\n",
    "m2.train()\n",
    "\n",
    "for i in range(5):\n",
    "    for x_batch, y_batch in trainLoader:\n",
    "        m2.train()\n",
    "        y_pred = m2(x_batch)\n",
    "        loss = loss_fun(y_pred, y_batch.float())\n",
    "        # print(\"pred: {} batch: {}\".format(y_pred, y_batch.float()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print('After {} epoch training loss is {}'.format(i, loss.item()))\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "print(m2(X_test_tensor))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:27:04.718600Z",
     "start_time": "2024-04-08T18:26:39.481400Z"
    }
   },
   "id": "6258f9117f683880",
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T18:27:04.728241Z",
     "start_time": "2024-04-08T18:27:04.721589Z"
    }
   },
   "id": "ba406fc79c22d061",
   "execution_count": 154
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
